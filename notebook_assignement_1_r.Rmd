---
title: "Pattern Recognition assignement in R"
output: html_notebook
---


The data file mnist.csv contains gray-scale images of hand-drawn digits, from zero through nine.

Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, 
indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.

The data set has 785 columns. The first column, called \"label\", is the digit that was written by the user. 
The rest of the columns contain the pixel-values of the associated image.

Each pixel column in the data set has a name like pixelx, where x is an integer between 0 and 783, inclusive. 
To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. 
Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).

For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.

Visually, if we omit the \"pixel\" prefix, the pixels make up the image like this:

000 001 002 ... 028\
029 030 031 ... 056\ 
...\
(its 28 x 28)

```{r}
#setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library("OpenImageR")
mnist.dat = read.csv("mnist.csv")
dim(mnist.dat)
#imageShow(matrix(as.numeric(mnist.dat[380,-1]),nrow=28,ncol=28,byrow=T))
```


Part 1---------------------------------------------\
Begin with an exploratory analysis of the data. 
Can you spot useless variables by looking at their summary statisitcs? 
Consider the class distribution: what percentage of cases would be classified correctly if we simply predict the majority class? 
Convert the first column (the digit) to a categorical variable using "as.factor" in R.
Report any findings from your exploratory analysis that you think are of interest.

```{r}
get_useless_indexes <- function(matrix){
  useless_pixels <- c() # these are always 1 value and dont change, so they are useless for informing our model
  for (i in 1:dim(matrix)[2]){
    unique_pixel_values <- unique(matrix[,i])
    if (length(unique_pixel_values)==1){ 
      useless_pixels <- c(useless_pixels,i)
    }
  }
  return(useless_pixels)
}

useless_mnist_pixels <- get_useless_indexes(mnist.dat[,-1])
mnist.dat[,1] <- as.factor(mnist.dat[,1]) # get all values from the first column (the number on the image)
y <- mnist.dat[,1] # this is just a vector of the classes now

number_of_entries <- dim(mnist.dat)[1] #first dimension of the dataset is the number of rows


majority_class = rev(sort(table(y)))[1]
correct_pred <- mnist.dat[mnist.dat[,1] == "1",]
correct_pred <- dim(correct_pred)[1]
accuracy = correct_pred / number_of_entries

```
How many times each class occurs:
```{r}
rev(sort(table(y))) # reverses a sorted table of how many times each class (factor) occurs
```

Accuracy with majority classifier:
```{r}
accuracy
```
Useless pixels (indexes, to get pixel subtract 1):
```{r}
useless_mnist_pixels
```



Part 2---------------------------------------------\
Derive from the raw pixel data a feature that quantifies "how much ink" a digit costs.
Report the average and standard deviation of this feature within each class. 
If you look at these statistics, can you see which pairs of classes can be distinguished well, and which pairs will be hard to distinguish using this feature?
  Hint: Use the R function "tapply" to compute the mean and standard deviation per digit. 
If your feature is called "ink", then "tapply(ink,mnist.dat[,1],mean)" will compute the mean value of ink for each digit.

```{r}
library(nnet)

ink_sum <- apply(mnist.dat[,-1],MARGIN=1,FUN=sum)  # Margin means do function on every row
ink_mean <- apply(mnist.dat[,-1],MARGIN=1,FUN=mean)  # Margin means do function on every row
ink_sd <- apply(mnist.dat[,-1],MARGIN=1,FUN=sd)  # Margin means do function on every row
# add the means, sd, for each class (unique numbers)
ink_scaled <- rep(c(0),length(ink_sum))
mnist.dat <- cbind(ink_sum,ink_mean,ink_sd,mnist.dat)


digit_sd <- aggregate(mnist.dat$ink_sum,by=list(mnist.dat$label),FUN=sd )
digit_mean <- aggregate(mnist.dat$ink_sum,by=list(mnist.dat$label),FUN=mean )
```
Digit mean:
```{r}
digit_mean
```


Digit Standard Deviation:
```{r}
digit_sd
```



Using only the ink feature, fit a multinomial logit model and evaluate,
by looking at the confusion matrix, how well this model can distinguish between the different classes.
Since in this part of the assignment we only consider very simple models, 
you may use the complete data set both for training and evaluation.

```{r}
mnist.dat$ink_scaled <- scale(mnist.dat$ink_sum,scale=max(mnist.dat$ink_sum),center=FALSE)

new_df <- data.frame(mnist.dat$label,mnist.dat$ink_scaled)
colnames(new_df) <- c("label","ink_scaled")

multinom <- multinom(new_df$label ~ new_df$ink_scaled)
multinom.pred <- predict(multinom,new_df$ink_scaled)
table(new_df$label,multinom.pred)
```


```{r}
library(MLmetrics)
# Accuracy:
print(sum(diag(table(new_df$label,multinom.pred)))/sum(table(new_df$label,multinom.pred)))
# accuracy 22%
#F1 score
F1_Score(multinom.pred,new_df$label)
# f1 score 0.351 
```

For example, how well can the model distinguish between the digits 
"1" and "8"? And how well between "3" and "8"? 
Use the R function "scale" to scale your feature before you apply "multinom" to fit the multinomial logit model.


ANSWER:\
The model is relatively good at detecting 0's and 1's, it has a hard time distinguishing between the other numbers though, with sometimes close to equal probabilities across the board.

I think this is because the means and standard deviations for digits 4 5 and 6 are somewhat simillar, so based on the sum ink feature alone it cannot distinguish between them and a 7, that is also simillar but occurs more often. In general when the values are simillar it seems to pick the most likely number based on the dataset.

the model cannot distinguish between 3 and 8 at all
 it didnt predict a 8 once, and predicted an 8 being a 3, more times than itpredicted a 3 correctly
 it can distinguish between 1 and 8 pretty well, it incorrectly predicted
 an 8 being a 1 140 times, but predicted 1 correctly 2856 times

Part 3 -------------------------------------------\
In addition to "ink", come up with one other feature, and explain why you think it might discriminate well between the digits. Your report should contain an unambiguous description of how the feature is derived from the raw data. Perform the same analysis for this new feature as you did for the ink feature.


I decided to use a canny edge detector on the images, with the hough transform to find straight lines, then because we are limited to only 1 feature (1 column) i calculate the ink sum for those new images that represent the detected straight lines on the image
```{r}
library(magick)
library(png)
dim(mnist.dat[15,5:788])

test_image <- matrix(as.numeric(mnist.dat[7,5:788]/255),nrow=28,ncol=28,byrow=T) # get an 2d 28 x 28 matrix representing the 15th image

test_image_object <- writePNG(test_image) # load the matrix into an image like object 

# To check what the image looks like, paste this into console
#imageShow(matrix(as.numeric(mnist.dat[15,5:788]),nrow=28,ncol=28,byrow=T))
```

What the edge detector output looks like for the test image
```{r}
edges <- image_canny(image_read(test_image_object),geometry="0x1+10%+30%") # Run canny edge detector, the geometry is using the default parameter
edges
```

Below we use hough line transform to detect straight lines in an image,
I fitted the geometry parameter until the output was detecting reasonably straight lines in the 28x28 image)
(what the output looks like is below)
```{r}
hough <- image_hough_draw(edges,geometry="50x50+11",size=0.3,color="white",bg = "black")
gray_image <- image_convert(hough,type="grayscale")
gray_image_data <- as.numeric(image_data(gray_image)) # this function converts the image to a matrix, like the original mnist data, but with lower value ranges (0 to 1) for pixels
gray_image
```



```{r}
# this is just the above code in 1 block to see what different images look like
index_of_image <- 3
test_image <- matrix(as.numeric(mnist.dat[index_of_image,5:788]/255),nrow=28,ncol=28,byrow=T) 
test_image_object <- writePNG(test_image)
edges <- image_canny(image_read(test_image_object),geometry="0x1+10%+30%")
edges
hough <- image_hough_draw(edges,geometry="50x50+11",size=0.3,color="white",bg = "black")
gray_image <- image_convert(hough,type="grayscale")
gray_image
```

Does the above for each image, gets their straight line images, adds them to new mnist like matrix

This took 50 minutes on my machine, but this will be a secret

```{r}
if(!(file.exists("line_image_matrix.rds"))){
  line_image_matrix <- array(c(0),dim = c(42000,28*28))
  pb = txtProgressBar(min = 1, max = nrow(mnist.dat), style = 3) 
  
  
  for (image_index in 1:nrow(mnist.dat)){
    image_mat <- matrix(as.numeric(mnist.dat[image_index,5:788]/255),nrow=28,ncol=28,byrow=T)
    image_object <- writePNG(image_mat)
    edges <- image_canny(image_read(image_object),geometry="0x1+10%+30%")
    hough <- image_hough_draw(edges,geometry="50x50+11",size=0.3,color="white",bg = "black")
    gray_image <- image_convert(hough,type="grayscale")
    gray_image_data <- t(as.numeric(image_data(gray_image))[,,1]) # make it 2D
    line_image_matrix[image_index,] <- as.vector(gray_image_data)
    if(image_index %% 100 == 0){
        setTxtProgressBar(pb,image_index)
        #browser()
    }
  
  }
  close(pb)
  #save file so it doesnt have to be calculated again
  saveRDS(line_image_matrix, file = "line_image_matrix.rds")
  print("done")
}else{
  line_image_matrix <- readRDS("line_image_matrix.rds")
  print("done")
}
```

Calculate the ink sum again, but this time its on the images that represent detected straight lines on the mnist images. This, combined with the original ink sum feature of the images will make differentiating between them easier.
```{r}
ink_sum <- apply(line_image_matrix,MARGIN=1,FUN=sum) 
straight_line_df <- cbind(ink_sum,y,line_image_matrix)
```
Mean of these new straight images
```{r}
straightness_sd <- aggregate(straight_line_df[,1],by=list(straight_line_df[,2]),FUN=sd )
straightness_mean <- aggregate(straight_line_df[,1],by=list(straight_line_df[,2]),FUN=mean )
straightness_mean

```
SD:
```{r}
straightness_sd
```
Part 4----------------------------------------------------\

Now we fit it to the multinomial classifier:
```{r}
ink_scaled <- scale(ink_sum,scale=max(ink_sum))

new_df <- data.frame(mnist.dat$label,mnist.dat$ink_scaled,ink_scaled)
colnames(new_df) <- c("label","ink_scaled","straight_line_scaled")

multinom <- multinom(new_df$label ~ ., data = new_df)
multinom.pred <- predict(multinom,new_df[,-1])
table(new_df$label,multinom.pred)
```
```{r}
# Accuracy:
print(sum(diag(table(new_df$label,multinom.pred)))/sum(table(new_df$label,multinom.pred)))
# Accuracy is 28%
# F1 score
F1_Score(multinom.pred,new_df$label)
# F1 score is 0.361
```

The accuracy is somewhat better than using only the ink sum feature (22% vs 28%), in most cases, out of all the predictions for a certain image, the correct class had the most counts out of all other classes (0,1,2,3,5,7,8,9)
4 was getting confused with 5, the straight line detector produces a simillar amount of straight edges for both usually.
6 was getting confused with 8,3 and 4, because they produce simillar straight line detection images, they also look simillar.


Overall the results are considerably better than our first prediction, with the predictor still confusing 4's with 5's and 6's with 8's 3's and 4's.

The standard deviation of the straight line ink used feature is also high, which means that the values are often spread far out around the mean and vary inside each number class. This would mean that the geometry parameter of the hough image transform function might need further tweaking, but since it takes a long time to run on every image in mnist, this will not be done now.



Part 5 ----------------------------------------\
In this part we use the 784 raw pixel values themselves as features.

First we downsample the images, because if we dont, it takes too long to train, we remove unnecessary columns from before too (ink features), then save it as an object again
```{r}


if(!(file.exists("preprocessed_data.rds"))){
preprocessed_data <- array(c(0),dim = c(nrow(mnist.dat),14*14))

pb = txtProgressBar(min = 1, max = nrow(mnist.dat), style = 3) 
for (index in 1:nrow(mnist.dat)){
 preprocessed_data[index,] <- down_sample_image(matrix(as.numeric(mnist.dat[index,-c(ncol(mnist.dat),4,3,2,1)]),nrow=28,ncol=28,byrow=T),2)
 setTxtProgressBar(pb,index)
}
close(pb)

saveRDS(preprocessed_data, file = "preprocessed_data.rds")
print("done")
}else{
  preprocessed_data <- readRDS("preprocessed_data.rds")
  print("done")
}

```

Split the data into test and train set then split the train set into folds for cross validation
```{r}
library(glmnet)
library(e1071)
library(nnet)

#cv.glmnet
#tune.svm
#tune.nnet

#useless_pixels <- get_useless_indexes(mnist.dat)
preprocessed_data_df <- data.frame(mnist.dat$label,preprocessed_data)

#imageShow(matrix(as.numeric(preprocessed_data_df[1,-1]),nrow=14,ncol=14,byrow=T)) # to check that it worked

#imageShow(matrix(as.numeric(mnist.dat[1,-c(1,2,3,4,ncol(mnist.dat))]),nrow=28,ncol=28,byrow=T)) # to check that it worked

set.seed(30)
rows <- sample(nrow(mnist.dat)) # randomly shuffle dataset
preprocessed_data_df <- preprocessed_data_df[rows,]

test_set <- preprocessed_data_df[5001:nrow(mnist.dat),]
train_set <- preprocessed_data_df[1:5000,]
```


```{r}
#---------------------------------------
# MULTINOMIAL LOGIT WITH LASSO
if(!(file.exists("preprocessed_lasso.rds"))){
lasso <- cv.glmnet(as.matrix(train_set[,-1]),train_set[,1],family="multinomial",type.measure="class", trace.it=TRUE, nfolds=4)
saveRDS(lasso, file = "preprocessed_lasso.rds")
}else{
    lasso <- readRDS("preprocessed_lasso.rds")
}
plot(lasso)

# The best lambda value with 4 fold corss validation is 10^-10
# this value minimizes the missclassification error

lasso.pred <- predict(lasso,as.matrix(test_set[,-1]),type="class")
lasso.pred.confmat <- table(test_set[,1],lasso.pred)
sum(diag(lasso.pred.confmat))/sum(lasso.pred.confmat)
# The accuracy is 98.57%
#F1 Score
F1_Score(test_set[,1],lasso.pred)
# 0.997 f1 score
#---------------------------------------
```


```{r}
#---------------------------------------
# SUPPORT VECTOR MACHINE
constant_pixel_indexes <- get_useless_indexes(train_set)
# I remove the constant pixels here, since they dont change, they are not useful to the SVM
svm <- svm(train_set[,-c(1,constant_pixel_indexes)],train_set[,1])

svm.pred <- predict(svm,test_set[,-c(1,constant_pixel_indexes)])
table(test_set[,1],svm.pred)

#Accuracy
sum(diag(table(test_set[,1],svm.pred)))/nrow(test_set)
# The accuracy is 95.2%
#F1 Score
F1_Score(test_set[,1],svm.pred)
# 0.982 F1 score



if(!(file.exists("preprocessed_svm_tuned.rds"))){
# try out the different regularizations (costs from 1 to 10)
svm.tune <- tune.svm(train_set[,-c(1,constant_pixel_indexes)],train_set[,1], cost=1:5)
saveRDS(svm.tune, file = "preprocessed_svm_tuned.rds")
}else{
    svm.tune <- readRDS("preprocessed_svm_tuned.rds")
}


svm.tuned <- tune.svm(train_set[,-c(1,constant_pixel_indexes)],train_set[,1], cost=1)

svm.tuned.pred <- predict(svm.tuned,test_set[,-c(1,constant_pixel_indexes)])
table(test_set[,1],svm.tuned.pred)

#Accuracy (tuned)
sum(diag(table(test_set[,1],svm.tuned.pred)))/nrow(test_set)
# The accuracy is %
#F1 Score (tuned)
F1_Score(test_set[,1],svm.tuned.pred)
#  F1 score

#---------------------------------------
```


```{r}
#---------------------------------------
# FEED FORWARD NEURAL NETWORK 
# tune.nnet in nnet
weight_decay_ranges <- 10^c(-1,-2,-3,-4,-5,-6,-7)
size_ranges <- 1:3

if(!(file.exists("preprocessed_nn_tuned.rds"))){
# try out the different regularizations (costs from 1 to 10)
tmodel <- tune.nnet(mnist.dat.label ~ .,data=train_set,size=1:5,decay=weight_decay_ranges)

saveRDS(tmodel, file = "preprocessed_nn_tuned.rds")
}else{
    tmodel <- readRDS("preprocessed_nn_tuned.rds")
}

#---------------------------------------



```

