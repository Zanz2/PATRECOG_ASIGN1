---
title: "R Notebook"
output: html_notebook
---

Patrecog assignement in R

The data file mnist.csv contains gray-scale images of hand-drawn digits, from zero through nine.

Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, 
indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.

The data set has 785 columns. The first column, called \"label\", is the digit that was written by the user. 
The rest of the columns contain the pixel-values of the associated image.

Each pixel column in the data set has a name like pixelx, where x is an integer between 0 and 783, inclusive. 
To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. 
Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).

For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.

Visually, if we omit the \"pixel\" prefix, the pixels make up the image like this:

000 001 002 ... 028\
029 030 031 ... 056\ 
...\
(its 28 x 28)

```{r}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library("OpenImageR")
mnist.dat = read.csv("mnist.csv")
dim(mnist.dat)
imageShow(matrix(as.numeric(mnist.dat[380,-1]),nrow=28,ncol=28,byrow=T))
```


Part 1---------------------------------------------
Begin with an exploratory analysis of the data. 
Can you spot useless variables by looking at their summary statisitcs? 
Consider the class distribution: what percentage of cases would be classified correctly if we simply predict the majority class? 
Convert the first column (the digit) to a categorical variable using "as.factor" in R.
Report any findings from your exploratory analysis that you think are of interest.

```{r}
get_useless_indexes <- function(matrix){
  useless_pixels <- c() # these are always 1 value and dont change, so they are useless for informing our model
  for (i in 1:dim(matrix)[2]){
    unique_pixel_values <- unique(matrix[,i])
    if (length(unique_pixel_values)==1){ 
      useless_pixels <- c(useless_pixels,i)
    }
  }
  return(useless_pixels)
}

useless_mnist_pixels <- get_useless_indexes(mnist.dat)
mnist.dat[,1] <- as.factor(mnist.dat[,1]) # get all values from the first column (the number on the image)
y <- mnist.dat[,1] # this is just a vector of the classes now
rev(sort(table(y))) # reverses a sorted table of how many times each class (factor) occurs
number_of_entries <- dim(mnist.dat)[1] #first dimension of the dataset is the number of rows


majority_class = rev(sort(table(y)))[1]
correct_pred <- mnist.dat[mnist.dat[,1] == "1",]
correct_pred <- dim(correct_pred)[1]
accuracy = correct_pred / number_of_entries
```
Accuracy with majority classifier:
```{r}
accuracy
```
Useless pixels (indexes, to get pixel subtract 1):
```{r}
useless_mnist_pixels
```



Part 2---------------------------------------------
Derive from the raw pixel data a feature that quantifies "how much ink" a digit costs.
Report the average and standard deviation of this feature within each class. 
If you look at these statistics, can you see which pairs of classes can be distinguished well, and which pairs will be hard to distinguish using this feature?
  Hint: Use the R function "tapply" to compute the mean and standard deviation per digit. 
If your feature is called "ink", then "tapply(ink,mnist.dat[,1],mean)" will compute the mean value of ink for each digit.

```{r}
library(nnet)

ink_sum <- apply(mnist.dat[,-1],MARGIN=1,FUN=sum)  # Margin means do function on every row
ink_mean <- apply(mnist.dat[,-1],MARGIN=1,FUN=mean)  # Margin means do function on every row
ink_sd <- apply(mnist.dat[,-1],MARGIN=1,FUN=sd)  # Margin means do function on every row
# add the means, sd, for each class (unique numbers)
ink_scaled <- rep(c(0),length(ink_sum))
mnist.dat <- cbind(ink_sum,ink_mean,ink_sd,mnist.dat)


digit_sd <- aggregate(mnist.dat$ink_sum,by=list(mnist.dat$label),FUN=sd )
digit_mean <- aggregate(mnist.dat$ink_sum,by=list(mnist.dat$label),FUN=mean )
```



Using only the ink feature, fit a multinomial logit model and evaluate,
by looking at the confusion matrix, how well this model can distinguish between the different classes.
Since in this part of the assignment we only consider very simple models, 
you may use the complete data set both for training and evaluation.

```{r}
mnist.dat$ink_scaled <- scale(mnist.dat$ink_sum,scale=max(mnist.dat$ink_sum),center=FALSE)

new_df <- data.frame(mnist.dat$label,mnist.dat$ink_scaled)
colnames(new_df) <- c("label","ink_scaled")

multinom <- multinom(new_df$label ~ new_df$ink_scaled)
multinom.pred <- predict(multinom,new_df$ink_scaled)
table(new_df$label,multinom.pred)
```



For example, how well can the model distinguish between the digits 
"1" and "8"? And how well between "3" and "8"? 
Use the R function "scale" to scale your feature before you apply "multinom" to fit the multinomial logit model.


ANSWER: the model cannot distinguish between 3 and 8 at all
 it didnt predict a 8 once, and predicted an 8 being a 3, more times than it 
 predicted a 3 correctly
 it can distinguish between 1 and 8 pretty well, it incorrectly predicted
 an 8 being a 1 140 times, but predicted 1 correctly 2856 times

```{r}

```

