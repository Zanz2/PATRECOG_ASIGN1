---
title: "Pattern Recognition assignement in R"
output: html_notebook
---


The data file mnist.csv contains gray-scale images of hand-drawn digits, from zero through nine.

Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, 
indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.

The data set has 785 columns. The first column, called \"label\", is the digit that was written by the user. 
The rest of the columns contain the pixel-values of the associated image.

Each pixel column in the data set has a name like pixelx, where x is an integer between 0 and 783, inclusive. 
To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. 
Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).

For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.

Visually, if we omit the \"pixel\" prefix, the pixels make up the image like this:

000 001 002 ... 028\
029 030 031 ... 056\ 
...\
(its 28 x 28)

```{r}
#setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library("OpenImageR")
mnist.dat = read.csv("mnist.csv")
dim(mnist.dat)
#imageShow(matrix(as.numeric(mnist.dat[380,-1]),nrow=28,ncol=28,byrow=T))
```


Part 1---------------------------------------------\
Begin with an exploratory analysis of the data. 
Can you spot useless variables by looking at their summary statisitcs? 
Consider the class distribution: what percentage of cases would be classified correctly if we simply predict the majority class? 
Convert the first column (the digit) to a categorical variable using "as.factor" in R.
Report any findings from your exploratory analysis that you think are of interest.

```{r}
get_useless_indexes <- function(matrix){
  useless_pixels <- c() # these are always 1 value and dont change, so they are useless for informing our model
  for (i in 1:dim(matrix)[2]){
    unique_pixel_values <- unique(matrix[,i])
    if (length(unique_pixel_values)==1){ 
      useless_pixels <- c(useless_pixels,i)
    }
  }
  return(useless_pixels)
}

useless_mnist_pixels <- get_useless_indexes(mnist.dat[,-1])
mnist.dat[,1] <- as.factor(mnist.dat[,1]) # get all values from the first column (the number on the image)
y <- mnist.dat[,1] # this is just a vector of the classes now

number_of_entries <- dim(mnist.dat)[1] #first dimension of the dataset is the number of rows


majority_class = rev(sort(table(y)))[1]
correct_pred <- mnist.dat[mnist.dat[,1] == "1",]
correct_pred <- dim(correct_pred)[1]
accuracy = correct_pred / number_of_entries

```
How many times each class occurs:
```{r}
rev(sort(table(y))) # reverses a sorted table of how many times each class (factor) occurs
```

Accuracy with majority classifier:
```{r}
accuracy
```
Useless pixels (indexes, to get pixel subtract 1):
```{r}
useless_mnist_pixels
```



Part 2---------------------------------------------\
Derive from the raw pixel data a feature that quantifies "how much ink" a digit costs.
Report the average and standard deviation of this feature within each class. 
If you look at these statistics, can you see which pairs of classes can be distinguished well, and which pairs will be hard to distinguish using this feature?
  Hint: Use the R function "tapply" to compute the mean and standard deviation per digit. 
If your feature is called "ink", then "tapply(ink,mnist.dat[,1],mean)" will compute the mean value of ink for each digit.

```{r}
library(nnet)

ink_sum <- apply(mnist.dat[,-1],MARGIN=1,FUN=sum)  # Margin means do function on every row
ink_mean <- apply(mnist.dat[,-1],MARGIN=1,FUN=mean)  # Margin means do function on every row
ink_sd <- apply(mnist.dat[,-1],MARGIN=1,FUN=sd)  # Margin means do function on every row
# add the means, sd, for each class (unique numbers)
ink_scaled <- rep(c(0),length(ink_sum))
mnist.dat <- cbind(ink_sum,ink_mean,ink_sd,mnist.dat)


digit_sd <- aggregate(mnist.dat$ink_sum,by=list(mnist.dat$label),FUN=sd )
digit_mean <- aggregate(mnist.dat$ink_sum,by=list(mnist.dat$label),FUN=mean )
```
Digit mean:
```{r}
digit_mean
```


Digit Standard Deviation:
```{r}
digit_sd
```



Using only the ink feature, fit a multinomial logit model and evaluate,
by looking at the confusion matrix, how well this model can distinguish between the different classes.
Since in this part of the assignment we only consider very simple models, 
you may use the complete data set both for training and evaluation.

```{r}
mnist.dat$ink_scaled <- scale(mnist.dat$ink_sum,scale=max(mnist.dat$ink_sum),center=FALSE)

new_df <- data.frame(mnist.dat$label,mnist.dat$ink_scaled)
colnames(new_df) <- c("label","ink_scaled")

multinom <- multinom(new_df$label ~ new_df$ink_scaled)
multinom.pred <- predict(multinom,new_df$ink_scaled, type="class")
table1 <- table(new_df$label,multinom.pred)
table1
```


```{r}
library(MLmetrics)
# Accuracy:
print(sum(diag(table(new_df$label,multinom.pred)))/sum(table(new_df$label,multinom.pred)))
# accuracy 22%
#F1 score
F1_Score(multinom.pred,new_df$label)
# f1 score 0.351 
```

For example, how well can the model distinguish between the digits 
"1" and "8"? And how well between "3" and "8"? 
Use the R function "scale" to scale your feature before you apply "multinom" to fit the multinomial logit model.


ANSWER:\
The model is relatively good at detecting 0's and 1's, it has a hard time distinguishing between the other numbers though, with sometimes close to equal probabilities across the board.

I think this is because the means and standard deviations for digits 4 5 and 6 are somewhat simillar, so based on the sum ink feature alone it cannot distinguish between them and a 7, that is also simillar but occurs more often. In general when the values are simillar it seems to pick the most likely number based on the dataset.

the model cannot distinguish between 3 and 8 at all
 it didnt predict a 8 once, and predicted an 8 being a 3, more times than itpredicted a 3 correctly
 it can distinguish between 1 and 8 pretty well, it incorrectly predicted
 an 8 being a 1 140 times, but predicted 1 correctly 2856 times

Part 3 -------------------------------------------\
In addition to "ink", come up with one other feature, and explain why you think it might discriminate well between the digits. Your report should contain an unambiguous description of how the feature is derived from the raw data. Perform the same analysis for this new feature as you did for the ink feature.


I decided to use a canny edge detector on the images, with the hough transform to find straight lines, then because we are limited to only 1 feature (1 column) i calculate the ink sum for those new images that represent the detected straight lines on the image
```{r}
library(magick)
library(png)
dim(mnist.dat[15,5:788])

test_image <- matrix(as.numeric(mnist.dat[7,5:788]/255),nrow=28,ncol=28,byrow=T) # get an 2d 28 x 28 matrix representing the 15th image

test_image_object <- writePNG(test_image) # load the matrix into an image like object 

# To check what the image looks like, paste this into console
#imageShow(matrix(as.numeric(mnist.dat[15,5:788]),nrow=28,ncol=28,byrow=T))
```

What the edge detector output looks like for the test image
```{r}
edges <- image_canny(image_read(test_image_object),geometry="0x1+10%+30%") # Run canny edge detector, the geometry is using the default parameter
edges
```

Below we use hough line transform to detect straight lines in an image,
I fitted the geometry parameter until the output was detecting reasonably straight lines in the 28x28 image)
(what the output looks like is below)
```{r}
hough <- image_hough_draw(edges,geometry="50x50+11",size=0.3,color="white",bg = "black")
gray_image <- image_convert(hough,type="grayscale")
gray_image_data <- as.numeric(image_data(gray_image)) # this function converts the image to a matrix, like the original mnist data, but with lower value ranges (0 to 1) for pixels
gray_image
```



```{r}
# this is just the above code in 1 block to see what different images look like
index_of_image <- 128
imageShow(matrix(as.numeric(mnist.dat[index_of_image,5:788]),nrow=28,ncol=28,byrow=T))
test_image <- matrix(as.numeric(mnist.dat[index_of_image,5:788]/255),nrow=28,ncol=28,byrow=T) 
test_image_object <- writePNG(test_image)
edges <- image_canny(image_read(test_image_object),geometry="0x1+10%+30%")
edges
hough <- image_hough_draw(edges,geometry="9x9+12",size=0.3,color="white",bg = "black")
gray_image <- image_convert(hough,type="grayscale")
gray_image
```

Does the above for each image, gets their straight line images, adds them to new mnist like matrix

This took 50 minutes on my machine, but this will be a secret

```{r}
if(!(file.exists("line_image_matrix.rds"))){
  line_image_matrix <- array(c(0),dim = c(42000,28*28))
  pb = txtProgressBar(min = 1, max = nrow(mnist.dat), style = 3) 
  
  
  for (image_index in 1:nrow(mnist.dat)){
    image_mat <- matrix(as.numeric(mnist.dat[image_index,5:788]/255),nrow=28,ncol=28,byrow=T)
    image_object <- writePNG(image_mat)
    edges <- image_canny(image_read(image_object),geometry="0x1+10%+30%")
    hough <- image_hough_draw(edges,geometry="9x9+12",size=0.3,color="white",bg = "black")
    gray_image <- image_convert(hough,type="grayscale")
    gray_image_data <- t(as.numeric(image_data(gray_image))[,,1]) # make it 2D
    line_image_matrix[image_index,] <- as.vector(gray_image_data)
    if(image_index %% 100 == 0){
        setTxtProgressBar(pb,image_index)
        #browser()
    }
  
  }
  close(pb)
  #save file so it doesnt have to be calculated again
  saveRDS(line_image_matrix, file = "line_image_matrix.rds")
  print("done")
}else{
  line_image_matrix <- readRDS("line_image_matrix.rds")
  print("done")
}
```

Calculate the ink sum again, but this time its on the images that represent detected straight lines on the mnist images. This, combined with the original ink sum feature of the images will make differentiating between them easier.
```{r}
ink_sum <- apply(line_image_matrix,MARGIN=1,FUN=sum) 
straight_line_df <- cbind(ink_sum,y,line_image_matrix)
```
Mean of these new straight images
```{r}
straightness_sd <- aggregate(straight_line_df[,1],by=list(straight_line_df[,2]),FUN=sd )
straightness_mean <- aggregate(straight_line_df[,1],by=list(straight_line_df[,2]),FUN=mean )
straightness_mean

```
SD:
```{r}
straightness_sd
```
Part 4----------------------------------------------------\

Now we fit it to the multinomial classifier:
```{r}
ink_straight_scaled <- scale(ink_sum,scale=max(ink_sum))

new_df <- data.frame(mnist.dat$label,mnist.dat$ink_scaled,ink_straight_scaled)
colnames(new_df) <- c("label","ink_scaled","ink_straight_scaled")

multinom <- multinom(label ~ ., data = new_df)
multinom.pred <- predict(multinom,new_df[,-1], type="class")
table2 <- table(new_df$label,multinom.pred)
table2
```
```{r}
# Accuracy:
print(sum(diag(table(new_df$label,multinom.pred)))/sum(table(new_df$label,multinom.pred)))
# Accuracy 
# F1 score
F1_Score(multinom.pred,new_df$label)
```

The accuracy is somewhat better than using only the ink sum feature (22% vs 28%), in most cases, out of all the predictions for a certain image, the correct class had the most counts out of all other classes (0,1,2,3,5,7,8,9)
4 was getting confused with 5, the straight line detector produces a simillar amount of straight edges for both usually.
6 was getting confused with 8,3 and 4, because they produce simillar straight line detection images, they also look simillar.


Overall the results are considerably better than our first prediction, with the predictor still confusing 4's with 5's and 6's with 8's 3's and 4's.

The standard deviation of the straight line ink used feature is also high, which means that the values are often spread far out around the mean and vary inside each number class. This would mean that the geometry parameter of the hough image transform function might need further tweaking, but since it takes a long time to run on every image in mnist, this will not be done now.



Part 5 ----------------------------------------\
In this part we use the 784 raw pixel values themselves as features.

First we downsample the images, because if we dont, it takes too long to train, we remove unnecessary columns from before too (ink features), then save it as an object again
```{r}

mnist.dat = read.csv("mnist.csv")
if(!(file.exists("preprocessed_data.rds"))){
preprocessed_data <- array(c(0),dim = c(nrow(mnist.dat),14*14))

pb = txtProgressBar(min = 1, max = nrow(mnist.dat), style = 3) 
for (index in 1:nrow(mnist.dat)){
 preprocessed_data[index,] <- down_sample_image(matrix(as.numeric(mnist.dat[index,-1]),nrow=28,ncol=28,byrow=T),2)
 setTxtProgressBar(pb,index)
}
close(pb)

saveRDS(preprocessed_data, file = "preprocessed_data.rds")
print("done")
}else{
  preprocessed_data <- readRDS("preprocessed_data.rds")
  print("done")
}

```

Makes a new mnist look alike dataframe that is 14x14 instead of 28x28 (scaled down) for speed
```{r}
library(glmnet)
library(e1071)
library(nnet)

preprocessed_data <- data.frame(preprocessed_data)
if(ncol(preprocessed_data)<197){
  preprocessed_data <- cbind(mnist.dat$label,preprocessed_data)
}
preprocessed_data[,1] <- as.factor(preprocessed_data[,1])
colnames(preprocessed_data)[1] <- "label"

set.seed(30)
rows <- sample(nrow(preprocessed_data)) # randomly shuffle dataset
preprocessed_data <- preprocessed_data[rows,]

test_set <- preprocessed_data[5001:nrow(preprocessed_data),]
train_set <- preprocessed_data[1:5000,]
colnames(train_set)[1] <- "label"
colnames(test_set)[1] <- "label"
```
Runs cross validation with the multinomial logit with lasso regularization, then plots a graph. The first dotted line represents the lambda value that minimizes the missclasification error, the second one one that minimizes the missclassification error with a trade-off for less features used (one standard deviation from the one that minimizes missclasification error). A lamba value of "-7" means 10^-7.

```{r}
#---------------------------------------
# MULTINOMIAL LOGIT WITH LASSO
constant_pixel_indexes <- get_useless_indexes(train_set)
# the constant pixels are ignored anyway, it makes no difference (tested)
if(!(file.exists("preprocessed_lasso.rds"))){
lasso <- cv.glmnet(as.matrix(train_set[,-c(1,constant_pixel_indexes)]),train_set[,1],family="multinomial",type.measure="class", trace.it=TRUE, nfolds=4)
saveRDS(lasso, file = "preprocessed_lasso.rds")
}else{
    lasso <- readRDS("preprocessed_lasso.rds")
}

# The best lambda value with 4 fold corss validation is 10^-7
# this value minimizes the missclassification error

plot(lasso)
```

Here we plot a confusion matrix and print accuracy and f1 score for the multinomial logistic regression model.
```{r}
lasso.pred <- predict(lasso,as.matrix(test_set[,-c(1,constant_pixel_indexes)]),type="class")
lasso.pred.confmat <- table(test_set[,1],lasso.pred)
table3 <- lasso.pred.confmat
print(lasso.pred.confmat)
#Accuracy
sum(diag(lasso.pred.confmat))/sum(lasso.pred.confmat)

#F1 Score
F1_Score(test_set[,1],lasso.pred)

#---------------------------------------
```
We plot a basic SVM model below, with standard parameters, and standard scaling, with columns with constants removed, the confusion matrix with f1 score and accuracy are shown below.

```{r}
#---------------------------------------
# SUPPORT VECTOR MACHINE
constant_pixel_indexes <- get_useless_indexes(train_set)
# I remove the constant pixels here, since they dont change, they are not useful to the SVM
svm <- svm(train_set[,-c(1,constant_pixel_indexes)],train_set[,1])

svm.pred <- predict(svm,test_set[,-c(1,constant_pixel_indexes)], type="class")
#print(table(test_set[,1],svm.pred))
#table4 <- table(test_set[,1],svm.pred)
#table4
#Accuracy
#print(sum(diag(table(test_set[,1],svm.pred)))/nrow(test_set))

#F1 Score
#print(F1_Score(test_set[,1],svm.pred))
```
Now we use tune, to do cross validation, and analyse different svms with cost parameter ranges that are shown in the costs vector, we also scale the values on our own, because different folds might have different constant values in each subset, so we just scale our numbers from 0-1, and turn scaling in the function off.

```{r}
scaled_train_set <- cbind(train_set$label,(train_set[,-1]/255))
scaled_test_set <- cbind(test_set$label,(test_set[,-1]/255))
costs <- c(1,5,10,15,20,25,28,30,35,40,50,75,100,150)

colnames(scaled_train_set)[1] <- "label"
colnames(scaled_test_set)[1] <- "label"
if(!(file.exists("preprocessed_svm_tuned.rds"))){
# try out the different regularizations (costs from 1 to 10)
svm.tune <- tune.svm(label ~ .,data=scaled_train_set, cost=costs,scale=FALSE)
saveRDS(svm.tune, file = "preprocessed_svm_tuned.rds")
}else{
    svm.tune <- readRDS("preprocessed_svm_tuned.rds")
}
```
We now select this model and analyse it, as before, the confusion matrix with f1 score and accuracy are shown below. Along with a plot of how different parameters performed (error with different performance cost parameters)

```{r} 
plot(svm.tune$performances$cost,svm.tune$performances$error,xlab="Cost",ylab="Missclassifciation Error")
# now we select the best model
svm.tuned <- svm.tune$best.model



svm.tuned.pred <- predict(svm.tuned,scaled_test_set[,-1], type="class")
print(table(scaled_test_set[,1],svm.tuned.pred))
table5 <- table(scaled_test_set[,1],svm.tuned.pred)
table5
#Accuracy (tuned)
print(sum(diag(table(scaled_test_set[,1],svm.tuned.pred)))/nrow(scaled_test_set))
# The accuracy is %
#F1 Score (tuned)
print(F1_Score(scaled_test_set[,1],svm.tuned.pred))
#  F1 score


#---------------------------------------
```
We now train a feed forward neural network with parameters set to size = 4 and decay = 0.003 as a baseline.

```{r}
#---------------------------------------
# FEED FORWARD NEURAL NETWORK 
# tune.nnet in nnet

#Scaled values for neural network, changes original values since this is the last code block


ff_nn <- nnet(label ~ .,data=scaled_train_set,size=4,decay=0.03)
predicted <- predict(ff_nn,scaled_test_set[,-1], type="class")
table(scaled_test_set[,1],predicted)
table6 <- table(scaled_test_set[,1],predicted)
print(table6)
#Accuracy
print(sum(diag(table(scaled_test_set[,1],predicted)))/nrow(scaled_test_set))

#F1 Score 
print(F1_Score(scaled_test_set[,1],predicted))
```
Now we run cross validation, for decay ranges from 0.01 to 0.000001 and size ranges from 1 to 4, then we use the model with the least classification error on the test dataset, and again plot its confusion matrix and show its accuracy and f1 score. Selected parameters and performance of tested models is also shown below.

```{r}

weight_decay_ranges <- c(0.5,0.3,0.1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7) # the new weights dont produce a model better than the old ones
size_ranges <- 1:4

if(!(file.exists("preprocessed_nn_tuned.rds"))){
# try out the different regularizations (costs from 1 to 10)
tmodel <- tune.nnet(label ~ .,data=scaled_train_set,size=size_ranges,decay=weight_decay_ranges)

saveRDS(tmodel, file = "preprocessed_nn_tuned.rds")
}else{
    tmodel <- readRDS("preprocessed_nn_tuned.rds")
}

tmodel.best <- tmodel$best.model
predicted <- predict(tmodel.best,scaled_test_set[,-1], type="class")
table7 <- table(scaled_test_set[,1],predicted)
table7
#Accuracy (tuned)
print(sum(diag(table(scaled_test_set[,1],predicted)))/nrow(scaled_test_set))

#F1 Score (tuned)
print(F1_Score(scaled_test_set[,1],predicted))

plot(tmodel$performances$error, tmodel$performances$decay, xlab="Missclasification Error",ylab="Weight decay",sub ="Numbers represent size parameter")
text(tmodel$performances$error, tmodel$performances$decay, labels=tmodel$performances$size, cex= 0.7,pos=1)

#---------------------------------------


```

Part 6----------------------------------------------------------


Which classification method(s) produced the best classifier for this problem?

After step 3 you will have determined the best parameter settings for each algorithm, and you will have applied the algorithm with these parameter settings to the complete training sample to produce a model. Furthermore you will have estimated the error of these models on the remaining data. Now compare the accuracies of these three models, and perform a statistical test to see if there are any significant differences between their accuracies.

ANSWER: The plots and most of this is done aboove, regarding the statistical test, i use F1 score, not sure if thats it, but this can be expanded in the report

```{r}
# Because of 0 counts some of these dont work
library(stats)
library(MASS)

#multinom_vect <- c(sum(diag(table3)),sum(table3)-sum(diag(table3)))
#svm_vect <- c(sum(diag(table5)),sum(table5)-sum(diag(table5)))
#nnet_vect <- c(sum(diag(table7)),sum(table7)-sum(diag(table7)))

#test_set[,1]
#scaled_test_set[,1]

#lasso.pred
#svm.tuned.pred
#predicted

# modelA        correct  incorrect
# modelA        correct  incorrect

#               ModelB   ModelB
#              correct   correct
#              incorrect incorrect

# Quarters from left to right top down, square1, square2, square3, square4
# Square1 <- modelA.pred == scaled_test_set[,1] & modelB.pred == scaled_test_set[,1]
# Square2 <- modelB.pred == scaled_test_set[,1] & scaled_test_set[,1] != modelA.pred
# Square3 <- modelA.pred == scaled_test_set[,1] & scaled_test_set[,1] != modelB.pred
# Square4 <- modelB.pred != scaled_test_set[,1] & modelA.pred != scaled_test_set[,1]

# ModelA_ModelB_table <- array(c(Square1,Square2,Square3,Square4),dim=c(2,2))


modelA.pred <- lasso.pred
modelB.pred <- svm.tuned.pred

Square1 <- sum(modelA.pred == scaled_test_set[,1] & modelB.pred == scaled_test_set[,1], na.rm = TRUE)
Square2 <- sum(modelB.pred == scaled_test_set[,1] & scaled_test_set[,1] != modelA.pred, na.rm = TRUE)
Square3 <- sum(modelA.pred == scaled_test_set[,1] & scaled_test_set[,1] != modelB.pred, na.rm = TRUE)
Square4 <- sum(modelB.pred != scaled_test_set[,1] & modelA.pred != scaled_test_set[,1], na.rm = TRUE)

lasso_svm_table <- array(c(Square1,Square2,Square3,Square4),dim=c(2,2))

modelA.pred <- svm.tuned.pred
modelB.pred <- predicted

Square1 <- sum(modelA.pred == scaled_test_set[,1] & modelB.pred == scaled_test_set[,1], na.rm = TRUE)
Square2 <- sum(modelB.pred == scaled_test_set[,1] & scaled_test_set[,1] != modelA.pred, na.rm = TRUE)
Square3 <- sum(modelA.pred == scaled_test_set[,1] & scaled_test_set[,1] != modelB.pred, na.rm = TRUE)
Square4 <- sum(modelB.pred != scaled_test_set[,1] & modelA.pred != scaled_test_set[,1], na.rm = TRUE)

svm_nnet_table <- array(c(Square1,Square2,Square3,Square4),dim=c(2,2))


modelA.pred <- lasso.pred
modelB.pred <- predicted

Square1 <- sum(modelA.pred == scaled_test_set[,1] & modelB.pred == scaled_test_set[,1], na.rm = TRUE)
Square2 <- sum(modelB.pred == scaled_test_set[,1] & scaled_test_set[,1] != modelA.pred, na.rm = TRUE)
Square3 <- sum(modelA.pred == scaled_test_set[,1] & scaled_test_set[,1] != modelB.pred, na.rm = TRUE)
Square4 <- sum(modelB.pred != scaled_test_set[,1] & modelA.pred != scaled_test_set[,1], na.rm = TRUE)

lasso_nnet_table <- array(c(Square1,Square2,Square3,Square4),dim=c(2,2))
```


```{r}
print("Mcnemar test results ---------------------------")
mcnemar.test(lasso_svm_table)
mcnemar.test(svm_nnet_table)
mcnemar.test(lasso_nnet_table)
print("-----------------------------------------------")

```
```{r}
print("Fisher exact test results NOT USED SINCE ITS PAIRED DATA ---------------------------")
#fisher.test(lasso_svm_table)
#fisher.test(svm_nnet_table)
#fisher.test(lasso_nnet_table)
print("-----------------------------------------------")
```

